{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "773d3501",
   "metadata": {},
   "source": [
    "# Split text in chunks\n",
    "---\n",
    "\n",
    "Now that we have text from the PDFs, we need to split it into smaller chunks. This is important both to limit the context provided to downstream LLM tasks (thus improving performance and reducing costs) and to get easier-to-read chunks to display later on in the app.\n",
    "\n",
    "Instead of splitting by a hard limit of characters or specific separators, we'll go page by page and ask an LLM to provide semantically meaningful chunks. The purpose here is to have chunks that represent one specific idea or topic, which will be useful for other steps in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979ef66d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778f1ba4",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633f6f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display\n",
    "from tqdm.auto import tqdm\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a256481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from polids.config import settings\n",
    "from polids.utils import is_text_similar\n",
    "from polids.text_chunking.openai import OpenAITextChunker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5492fce9",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1de4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248cee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d90bbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360406b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_annotated_data_path = Path(\"data/portugal_2022/programs/\")\n",
    "human_annotated_md = human_annotated_data_path / \"livre.md\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede3fccd",
   "metadata": {},
   "source": [
    "## Get text from a PDF\n",
    "\n",
    "Here we're using human annotated text from a PDF instead of parsing it, so as to keep this notebook separate from the PDF parsing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86f5ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_content = human_annotated_md.read_text()\n",
    "display(Markdown(markdown_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb437405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process markdown content into semantic chunks that preserve document structure\n",
    "# Strategy: Keep each section header with its first paragraph for context,\n",
    "# then treat remaining paragraphs as individual chunks\n",
    "\n",
    "# Split content at section boundaries (level-2 headers after paragraph breaks)\n",
    "markdown_sections = re.split(r\"(?<=\\n\\n)(?=## )\", markdown_content)\n",
    "\n",
    "markdown_paragraphs = []\n",
    "for section in markdown_sections:\n",
    "    # Split section into individual paragraphs and clean whitespace\n",
    "    section_paragraphs = [paragraph.strip() for paragraph in section.split(\"\\n\\n\")]\n",
    "\n",
    "    if section_paragraphs:\n",
    "        # Preserve context by keeping header with its introduction paragraph\n",
    "        header_and_introduction = \"\\n\\n\".join(section_paragraphs[:2])\n",
    "        markdown_paragraphs.append(header_and_introduction)\n",
    "\n",
    "        # Process remaining paragraphs as individual chunks\n",
    "        for paragraph in section_paragraphs[2:]:\n",
    "            # Skip empty paragraphs and horizontal separators\n",
    "            if paragraph and paragraph != \"---\":\n",
    "                markdown_paragraphs.append(paragraph)\n",
    "\n",
    "# Return the final list of semantic document chunks\n",
    "markdown_paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86347e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(markdown_paragraphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390c0053",
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(text) for text in markdown_paragraphs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309ec9ef",
   "metadata": {},
   "source": [
    "## Split the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ea8548",
   "metadata": {},
   "source": [
    "### Get chunks from a page and its following page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9ff07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=settings.openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaf91b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticChunksPerPage(BaseModel):\n",
    "    \"\"\"\n",
    "    Model to represent semantic chunks of each individual page.\n",
    "    \"\"\"\n",
    "\n",
    "    chunks: list[str] = Field(\n",
    "        description=\"List of semantic chunks extracted from the page, exactly matching segments of the original Markdown text.\"\n",
    "    )\n",
    "    last_chunk_incomplete: bool = Field(\n",
    "        description=\"Boolean flag that is true if the last chunk's topic continues into the preview of the next page, false otherwise.\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36638863",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_outputs: list[SemanticChunksPerPage] = []\n",
    "# Iterate through each page of the document, extracting semantic chunks\n",
    "for idx, current_page_text in tqdm(\n",
    "    enumerate(markdown_paragraphs),\n",
    "    total=len(markdown_paragraphs),\n",
    "    desc=\"Processing pages\",\n",
    "):\n",
    "    if idx == len(markdown_paragraphs) - 1:\n",
    "        next_page_preview = \"\"\n",
    "    else:\n",
    "        # Provide the LLM with a tweet-sized preview of the next page to check if the last chunk is incomplete\n",
    "        next_page_text = markdown_paragraphs[idx + 1]\n",
    "        next_page_preview = next_page_text.split(\"\\n\\n\")[0][:280]\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        # Using the mini version for cheaper processing; setting a specific version for reproducibility\n",
    "        model=\"gpt-4.1-mini-2025-04-14\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are an AI assistant specialized in semantic text chunking for Markdown documents. Your task is to divide the provided <current_page_text> into a list of semantically coherent chunks, preserving all original Markdown formatting exactly.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"**Overall Goal:**\n",
    "Analyze the <current_page_text> (which is in Markdown format from a political manifesto) and split it into semantically coherent chunks. **Each chunk should ideally represent a distinct policy proposal, argument, thematic topic, or logical section typical of such documents.** This chunking is intended to facilitate downstream NLP analysis for understanding policy positions and informing voters. Perform this task objectively based on text structure and topic shifts, regardless of the political viewpoint expressed. Use the <next_page_preview> **only** to determine if the very last chunk of the <current_page_text> is semantically incomplete because its specific topic or proposal clearly continues into the preview text.\n",
    "\n",
    "**Analysis Process:**\n",
    "1.  **Carefully Analyze:** First, read and fully understand the entire <current_page_text> and the <next_page_preview> to grasp the context, policy flow, and document structure.\n",
    "2.  **Identify Semantic Breaks:** Determine logical breakpoints in the <current_page_text> based on shifts in **topic, introduction of new policy proposals, distinct arguments, or transitions between manifesto sections.**\n",
    "3.  **Use Markdown & Document Cues:** Treat Markdown elements (headings `#`, `##`; lists `*`, `-`, `1.`; paragraphs separated by blank lines; thematic breaks `---`) **and common manifesto structures (e.g., explicitly numbered proposals, thematic chapters/sections)** as strong indicators for potential chunk boundaries, but always prioritize the semantic flow of policy ideas or arguments. A single chunk can span multiple paragraphs or elements if they detail the *same* core proposal or argument.\n",
    "4.  **Check Final Chunk:** Evaluate if the *last* identified semantic unit (e.g., the end of a policy description) in <current_page_text> stops mid-thought and its specific topic clearly continues at the start of <next_page_preview>.\n",
    "5.  **Format Output:** Construct the required structured output according to the provided schema.\n",
    "\n",
    "**Critical Constraints:**\n",
    "- **Exact Markdown Preservation:** You MUST NOT alter the <current_page_text>. Preserve ALL original Markdown syntax, whitespace (spaces, tabs, newlines), and characters exactly. The concatenation of the output `chunks` MUST perfectly reconstruct the original <current_page_text> string.\n",
    "- **Language Agnostic:** Perform semantic analysis regardless of the text's language, preserving the original language within the chunks.\n",
    "\n",
    "<current_page_text>\n",
    "{current_page_text}\n",
    "</current_page_text>\n",
    "<next_page_preview>\n",
    "{next_page_preview}\n",
    "</next_page_preview>\"\"\",\n",
    "            },\n",
    "        ],\n",
    "        response_format=SemanticChunksPerPage,  # Specify the schema for the structured output\n",
    "        temperature=0,  # Low temperature should lead to less hallucination\n",
    "        seed=42,  # Fix the seed for reproducibility\n",
    "    )\n",
    "    chunks_output = completion.choices[0].message.parsed\n",
    "    assert isinstance(chunks_output, SemanticChunksPerPage), (\n",
    "        \"Output does not match the expected schema.\"\n",
    "    )\n",
    "    assert is_text_similar(\n",
    "        expected=current_page_text, actual=\"\\n\".join(chunks_output.chunks)\n",
    "    ), \"Output chunks do not reconstruct the original text.\"\n",
    "    chunk_outputs.append(chunks_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d21a1e",
   "metadata": {},
   "source": [
    "Currently, this solution takes around 13 minutes to process around 100 pages of text. It could be worth it to consider faster LLM services, such as Groq or Cerebras."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e0a5b59",
   "metadata": {},
   "source": [
    "### Merge (potentially) incomplete chunk from the end of a page with the first chunk of the next page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c81eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_chunks: list[str] = []\n",
    "carry_over_chunk: str | None = None\n",
    "\n",
    "total_pages = len(chunk_outputs)\n",
    "\n",
    "for i, page_data in enumerate(chunk_outputs):\n",
    "    # Make a copy to avoid modifying the input list's contents directly\n",
    "    current_chunks: list[str] = list(page_data.chunks)\n",
    "    is_last_incomplete: bool = page_data.last_chunk_incomplete\n",
    "\n",
    "    # Skip pages that resulted in no chunks\n",
    "    if not current_chunks:\n",
    "        # If there was a carry-over, it means the previous page ended\n",
    "        # mid-chunk, and this page is empty. The carry-over chunk\n",
    "        # should likely stand alone as the last chunk of the previous semantic unit.\n",
    "        if carry_over_chunk is not None:\n",
    "            final_chunks.append(carry_over_chunk)\n",
    "            carry_over_chunk = None  # Reset carry-over\n",
    "        continue  # Skip processing for this empty page\n",
    "\n",
    "    # 1. Handle carry-over from the *previous* page\n",
    "    if carry_over_chunk is not None:\n",
    "        # Prepend the carried-over text to the first chunk of the current page\n",
    "        current_chunks[0] = carry_over_chunk + \" \" + current_chunks[0]\n",
    "        carry_over_chunk = None  # Reset carry-over as it's now merged\n",
    "\n",
    "    # 2. Check if the *current* page's last chunk needs carrying over\n",
    "    # This happens if the flag is True AND it's not the very last page overall\n",
    "    is_last_page_overall = i == total_pages - 1\n",
    "\n",
    "    if is_last_incomplete and not is_last_page_overall:\n",
    "        # The last chunk of this page is incomplete and needs merging later.\n",
    "        # Set it aside as the next carry_over_chunk.\n",
    "        carry_over_chunk = current_chunks[-1]\n",
    "        # Add all chunks *except the last one* to the final list for now.\n",
    "        final_chunks.extend(current_chunks[:-1])\n",
    "    else:\n",
    "        # This page's last chunk is complete, OR it's the last page overall.\n",
    "        # Add all chunks from this page (potentially modified by step 1)\n",
    "        # to the final list.\n",
    "        final_chunks.extend(current_chunks)\n",
    "final_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e951da55",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(final_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20430ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "[len(text) for text in final_chunks]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3b1644",
   "metadata": {},
   "source": [
    "### Implemented solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0651f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunker = OpenAITextChunker()\n",
    "chunks = text_chunker.process(markdown_paragraphs)\n",
    "print(f\"Split into {len(chunks)} chunks:\")\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8097133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
