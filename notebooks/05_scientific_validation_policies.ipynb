{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a038bd6",
   "metadata": {},
   "source": [
    "# Scientific validation of policy proposals\n",
    "---\n",
    "Experimenting with web search APIs for scientific validation of policy proposals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f82055",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68012ac",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dbf819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Dict\n",
    "import json\n",
    "import backoff\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "from tqdm.auto import tqdm\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from openai import OpenAI\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dadd7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from polids.config import settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e316f2c6",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc968457",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Search the web for credible scientific context related to a given policy proposal, and determine its scientific validation.\n",
    "\n",
    "Identify and evaluate sources such as highly cited academic papers, randomized controlled trials (RCTs), and reputable news outlets with scientific grounding. Based on this research, provide a validation outcome and a justification.\n",
    "\n",
    "# Steps\n",
    "\n",
    "1. **Identify Keywords**: Extract main concepts and terms from the policy proposal to guide your search.\n",
    "2. **Conduct Web Search**: Use the identified keywords to search for scientific literature, credible reports, and analyses related to the policy.\n",
    "3. **Evaluate Sources**: Prioritize sources based on credibility, relevance, and citation count. Look for consensus among multiple credible sources to enhance reliability.\n",
    "4. **Synthesize Information**: Summarize the findings clearly indicating whether the scientific evidence supports or refutes the proposal.\n",
    "5. **Conclude Validation**: Determine if the policy is scientifically validated based on gathered evidence.\n",
    "6. **Provide Reasoning**: Articulate the reasoning based on the findings, citing key evidence.\n",
    "\n",
    "# Notes\n",
    "\n",
    "- Regardless of the original language of the proposal, the search should be conducted in English and the results should be presented in English.\n",
    "- Ensure the evaluation is grounded in current and credible scientific data.\n",
    "- Consider the strength and consensus of evidence rather than anecdotal or single-study claims.\n",
    "- If evidence is mixed, provide a balanced view in the reasoning string.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170cbe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_retries = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d2a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "perplexity_api_key = os.getenv(\"PERPLEXITY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93575d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_sources = [\n",
    "    # --- General Knowledge ---\n",
    "    # Sources offering broad, encyclopedic information.\n",
    "    \"wikipedia.org\",  # Crowd-sourced general knowledge encyclopedia\n",
    "    # --- Data Aggregators ---\n",
    "    # Platforms specializing in collecting, analyzing, and visualizing data on various topics.\n",
    "    \"ourworldindata.org\",  # Accessible global data visualization & analysis\n",
    "    # --- International Organizations ---\n",
    "    # Official websites of major international bodies providing data, reports, and policy guidelines.\n",
    "    \"oecd.org\",  # Organisation for Economic Co-operation and Development data & reports\n",
    "    \"un.org\",  # United Nations reports & policy guidelines (global issues)\n",
    "    \"worldbank.org\",  # World Bank global economic & development data\n",
    "    # --- Research & Policy Analysis Institutes ---\n",
    "    # Organizations focused on specific research areas, often influencing policy.\n",
    "    \"nber.org\",  # National Bureau of Economic Research (influential economics)\n",
    "    # --- Research Aggregators & Databases ---\n",
    "    # Platforms providing access to collections of academic research papers.\n",
    "    \"core.ac.uk\",  # Aggregator for open access research papers (multidisciplinary)\n",
    "    \"ncbi.nlm.nih.gov\",  # National Center for Biotechnology Information (biomedical literature)\n",
    "    \"arxiv.org\",  # Open access preprints (physics, math, CS, quantitative biology, etc.)\n",
    "    \"sci-hub.box\",  # Tool for accessing paywalled scientific papers (legality varies)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380113cc",
   "metadata": {},
   "source": [
    "## Load policies to validate\n",
    "We're going to start from manually defined policies, so as to avoid dependencies on previous steps of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed47a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "policies_to_validate = {\n",
    "    \"carbon_tax\": \"Implementing a carbon tax to reduce greenhouse gas emissions.\",\n",
    "    \"vaccines\": \"Mandatory vaccination for all school-aged children to prevent outbreaks of infectious diseases.\",\n",
    "    \"ubi\": \"Implementing universal basic income to address income inequality and support job displacement due to automation.\",\n",
    "    \"immigration_jobs\": \"Reducing immigration quotas to improve job opportunities for native citizens.\",\n",
    "    \"immigration_crime\": \"Blocking immigration from countries with different cultural backgrounds to reduce crime rates.\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe970012",
   "metadata": {},
   "source": [
    "## Define the output schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbefc42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScientificValidation(BaseModel):\n",
    "    is_policy_supported_by_sources: bool = Field(\n",
    "        description=\"Indicates whether the policy proposal has scientific backing or not.\"\n",
    "    )\n",
    "    is_validation_consensual_and_reliable: bool = Field(\n",
    "        description=\"Indicates whether the validation is based on a consensus of multiple reliable sources.\"\n",
    "    )\n",
    "    reasoning: str = Field(\n",
    "        description=\"A detailed explanation of the validation outcome, including key evidence and sources.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b51150",
   "metadata": {},
   "source": [
    "## Test different search APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e546428b",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9378de62",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=settings.openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7494e588",
   "metadata": {},
   "source": [
    "#### GPT 4o mini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0006d7e7",
   "metadata": {},
   "source": [
    "##### Low search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b81f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_validation_results = {\n",
    "    example_name: None for example_name in policies_to_validate.keys()\n",
    "}\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini-search-preview\",\n",
    "        web_search_options={\n",
    "            \"search_context_size\": \"low\",\n",
    "        },\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": example_policy,\n",
    "            },\n",
    "        ],\n",
    "        response_format=ScientificValidation,  # Specify the schema for the structured output\n",
    "    )\n",
    "    policy_validation_results[example_name] = completion.choices[0].message.parsed\n",
    "    assert isinstance(policy_validation_results[example_name], ScientificValidation), (\n",
    "        \"Output does not match the expected schema.\"\n",
    "    )\n",
    "    citations[example_name] = completion.choices[0].message.annotations\n",
    "policy_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4ae459",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef6108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(Markdown(policy_validation_results[example_name].reasoning))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fbd04a",
   "metadata": {},
   "source": [
    "Seems like GPT 4o mini with low search often doesn't use any search results at all. When it does, I'm only seeing two citations. This is not enough to validate a policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c150b0",
   "metadata": {},
   "source": [
    "##### Medium search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10fe4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_validation_results = {\n",
    "    example_name: None for example_name in policies_to_validate.keys()\n",
    "}\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini-search-preview\",\n",
    "        web_search_options={\n",
    "            \"search_context_size\": \"medium\",\n",
    "        },\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": example_policy,\n",
    "            },\n",
    "        ],\n",
    "        response_format=ScientificValidation,  # Specify the schema for the structured output\n",
    "    )\n",
    "    policy_validation_results[example_name] = completion.choices[0].message.parsed\n",
    "    assert isinstance(policy_validation_results[example_name], ScientificValidation), (\n",
    "        \"Output does not match the expected schema.\"\n",
    "    )\n",
    "    citations[example_name] = completion.choices[0].message.annotations\n",
    "policy_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c810fd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861a95a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(Markdown(policy_validation_results[example_name].reasoning))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d1d01b",
   "metadata": {},
   "source": [
    "Medium gets more sources for some of the samples, but still has some of them without any citations. This is not enough to validate a policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d7f3b8",
   "metadata": {},
   "source": [
    "##### High search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf02692",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_validation_results = {\n",
    "    example_name: None for example_name in policies_to_validate.keys()\n",
    "}\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini-search-preview\",\n",
    "        web_search_options={\n",
    "            \"search_context_size\": \"high\",\n",
    "        },\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": example_policy,\n",
    "            },\n",
    "        ],\n",
    "        response_format=ScientificValidation,  # Specify the schema for the structured output\n",
    "    )\n",
    "    policy_validation_results[example_name] = completion.choices[0].message.parsed\n",
    "    assert isinstance(policy_validation_results[example_name], ScientificValidation), (\n",
    "        \"Output does not match the expected schema.\"\n",
    "    )\n",
    "    citations[example_name] = completion.choices[0].message.annotations\n",
    "policy_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b895b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3a4006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(Markdown(policy_validation_results[example_name].reasoning))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8afb355",
   "metadata": {},
   "source": [
    "Still some samples without citations, even on high search context ðŸ‘ŽðŸ»"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45265eb4",
   "metadata": {},
   "source": [
    "#### GPT 4o (larger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0d5f76",
   "metadata": {},
   "source": [
    "##### Low search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f7dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_validation_results = {\n",
    "    example_name: None for example_name in policies_to_validate.keys()\n",
    "}\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-search-preview\",\n",
    "        web_search_options={\n",
    "            \"search_context_size\": \"low\",\n",
    "        },\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": example_policy,\n",
    "            },\n",
    "        ],\n",
    "        response_format=ScientificValidation,  # Specify the schema for the structured output\n",
    "    )\n",
    "    policy_validation_results[example_name] = completion.choices[0].message.parsed\n",
    "    assert isinstance(policy_validation_results[example_name], ScientificValidation), (\n",
    "        \"Output does not match the expected schema.\"\n",
    "    )\n",
    "    citations[example_name] = completion.choices[0].message.annotations\n",
    "policy_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d72ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d01e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(Markdown(policy_validation_results[example_name].reasoning))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820dd8fa",
   "metadata": {},
   "source": [
    "##### Medium search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3093c581",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_validation_results = {\n",
    "    example_name: None for example_name in policies_to_validate.keys()\n",
    "}\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-search-preview\",\n",
    "        web_search_options={\n",
    "            \"search_context_size\": \"medium\",\n",
    "        },\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": example_policy,\n",
    "            },\n",
    "        ],\n",
    "        response_format=ScientificValidation,  # Specify the schema for the structured output\n",
    "    )\n",
    "    policy_validation_results[example_name] = completion.choices[0].message.parsed\n",
    "    assert isinstance(policy_validation_results[example_name], ScientificValidation), (\n",
    "        \"Output does not match the expected schema.\"\n",
    "    )\n",
    "    citations[example_name] = completion.choices[0].message.annotations\n",
    "policy_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90368961",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554c3cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(Markdown(policy_validation_results[example_name].reasoning))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c86123",
   "metadata": {},
   "source": [
    "##### High search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab8982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_validation_results = {\n",
    "    example_name: None for example_name in policies_to_validate.keys()\n",
    "}\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-search-preview\",\n",
    "        web_search_options={\n",
    "            \"search_context_size\": \"high\",\n",
    "        },\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": example_policy,\n",
    "            },\n",
    "        ],\n",
    "        response_format=ScientificValidation,  # Specify the schema for the structured output\n",
    "    )\n",
    "    policy_validation_results[example_name] = completion.choices[0].message.parsed\n",
    "    assert isinstance(policy_validation_results[example_name], ScientificValidation), (\n",
    "        \"Output does not match the expected schema.\"\n",
    "    )\n",
    "    citations[example_name] = completion.choices[0].message.annotations\n",
    "policy_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df6c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9785b324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(Markdown(policy_validation_results[example_name].reasoning))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84dab79",
   "metadata": {},
   "source": [
    "Still some samples without citations, even on high search context and with the larger GPT 4o ðŸ‘ŽðŸ»"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a027885c",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77934787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_valid_json(response: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extracts and returns only the valid JSON part from a response object.\n",
    "\n",
    "    This function assumes that the response has a structure where the valid JSON\n",
    "    is included in the 'content' field of the first choice's message, after the\n",
    "    closing \"</think>\" marker. Any markdown code fences (e.g. ```json) are stripped.\n",
    "\n",
    "    Parameters:\n",
    "        response (dict): The full API response object.\n",
    "\n",
    "    Returns:\n",
    "        dict: The parsed JSON object extracted from the content.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If no valid JSON can be parsed from the content.\n",
    "    \"\"\"\n",
    "    # Navigate to the 'content' field; adjust if your structure differs.\n",
    "    content = response.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "\n",
    "    # Find the index of the closing </think> tag.\n",
    "    marker = \"</think>\"\n",
    "    idx = content.rfind(marker)\n",
    "\n",
    "    if idx == -1:\n",
    "        # If marker not found, try parsing the entire content.\n",
    "        try:\n",
    "            return json.loads(content)\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(\n",
    "                \"No </think> marker found and content is not valid JSON\"\n",
    "            ) from e\n",
    "\n",
    "    # Extract the substring after the marker.\n",
    "    json_str = content[idx + len(marker) :].strip()\n",
    "\n",
    "    # Remove markdown code fence markers if present.\n",
    "    if json_str.startswith(\"```json\"):\n",
    "        json_str = json_str[len(\"```json\") :].strip()\n",
    "    if json_str.startswith(\"```\"):\n",
    "        json_str = json_str[3:].strip()\n",
    "    if json_str.endswith(\"```\"):\n",
    "        json_str = json_str[:-3].strip()\n",
    "\n",
    "    try:\n",
    "        parsed_json = json.loads(json_str)\n",
    "        return parsed_json\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(\"Failed to parse valid JSON from response content\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188bce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "@backoff.on_exception(\n",
    "    backoff.expo,\n",
    "    ValidationError,\n",
    "    max_tries=max_retries,\n",
    "    max_time=60,\n",
    ")\n",
    "def search_on_perplexity(\n",
    "    policy: str,\n",
    "    model_name: str,\n",
    "    search_context_size: str = None,\n",
    "    system_prompt: str = system_prompt,\n",
    "    perplexity_api_key: str = perplexity_api_key,\n",
    "    allowed_sources: list = None,\n",
    ") -> tuple[ScientificValidation, list]:\n",
    "    \"\"\"\n",
    "    Search for scientific validation of a policy proposal using Perplexity AI.\n",
    "\n",
    "    Args:\n",
    "        policy (str): The policy proposal to validate.\n",
    "        model_name (str): The model name to use for the search.\n",
    "        search_context_size (str, optional): The size of the search context (low, medium, high).\n",
    "        system_prompt (str): The system prompt for the model.\n",
    "        perplexity_api_key (str): The API key for Perplexity AI.\n",
    "        allowed_sources (list, optional): A list of allowed sources for the search.\n",
    "\n",
    "    Returns:\n",
    "        tuple[ScientificValidation, list]: A tuple containing the validation result and citations.\n",
    "    \"\"\"\n",
    "    request_payload = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": policy},\n",
    "        ],\n",
    "        \"response_format\": {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\"schema\": ScientificValidation.model_json_schema()},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if allowed_sources:\n",
    "        # Only search on the allowed sources\n",
    "        request_payload[\"search_domain_filter\"] = allowed_sources\n",
    "\n",
    "    if search_context_size:\n",
    "        # Define how many sources to use for the search\n",
    "        assert search_context_size in [\"low\", \"medium\", \"high\"], (\n",
    "            f\"Invalid search context size: {search_context_size}. \"\n",
    "            \"Must be one of: low, medium, high.\"\n",
    "        )\n",
    "        request_payload[\"web_search_options\"] = {\n",
    "            \"search_context_size\": search_context_size\n",
    "        }\n",
    "\n",
    "    response = requests.post(\n",
    "        \"https://api.perplexity.ai/chat/completions\",\n",
    "        headers={\"Authorization\": f\"Bearer {perplexity_api_key}\"},\n",
    "        json=request_payload,\n",
    "    ).json()\n",
    "\n",
    "    citations = response.get(\"citations\", [])\n",
    "    response_content = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    if (\"reasoning\" in model_name) or (\"<think>\" in response_content):\n",
    "        # Extract the valid JSON part from the response content\n",
    "        json_content = extract_valid_json(response)\n",
    "        # Parse the JSON content into the ScientificValidation model\n",
    "        parsed_content = ScientificValidation.model_validate(json_content)\n",
    "    else:\n",
    "        # Parse the string content into the ScientificValidation model\n",
    "        parsed_content = ScientificValidation.model_validate_json(response_content)\n",
    "\n",
    "    return parsed_content, citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e614c0ad",
   "metadata": {},
   "source": [
    "#### Sonar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7328b30d",
   "metadata": {},
   "source": [
    "##### Low search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f81589",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_validation_results = {\n",
    "    example_name: None for example_name in policies_to_validate.keys()\n",
    "}\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    policy_validation_results[example_name], citations[example_name] = (\n",
    "        search_on_perplexity(\n",
    "            policy=example_policy,\n",
    "            model_name=\"sonar\",\n",
    "            search_context_size=\"low\",\n",
    "            system_prompt=system_prompt,\n",
    "            perplexity_api_key=perplexity_api_key,\n",
    "            allowed_sources=allowed_sources,\n",
    "        )\n",
    "    )\n",
    "policy_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cc2184",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57088c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(Markdown(policy_validation_results[example_name].reasoning))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dc05a3",
   "metadata": {},
   "source": [
    "Wow this already worked very well, even in the cheapest setting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6046c589",
   "metadata": {},
   "source": [
    "##### Medium search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189feb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_validation_results = {\n",
    "    example_name: None for example_name in policies_to_validate.keys()\n",
    "}\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    policy_validation_results[example_name], citations[example_name] = (\n",
    "        search_on_perplexity(\n",
    "            policy=example_policy,\n",
    "            model_name=\"sonar\",\n",
    "            search_context_size=\"medium\",\n",
    "            system_prompt=system_prompt,\n",
    "            perplexity_api_key=perplexity_api_key,\n",
    "            allowed_sources=allowed_sources,\n",
    "        )\n",
    "    )\n",
    "policy_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c274309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd3ef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(Markdown(policy_validation_results[example_name].reasoning))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dde98af",
   "metadata": {},
   "source": [
    "##### High search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb287b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_validation_results = {\n",
    "    example_name: None for example_name in policies_to_validate.keys()\n",
    "}\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    policy_validation_results[example_name], citations[example_name] = (\n",
    "        search_on_perplexity(\n",
    "            policy=example_policy,\n",
    "            model_name=\"sonar\",\n",
    "            search_context_size=\"high\",\n",
    "            system_prompt=system_prompt,\n",
    "            perplexity_api_key=perplexity_api_key,\n",
    "            allowed_sources=allowed_sources,\n",
    "        )\n",
    "    )\n",
    "policy_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ff672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f08d4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(Markdown(policy_validation_results[example_name].reasoning))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c298c4",
   "metadata": {},
   "source": [
    "#### Sonar Pro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3526d97b",
   "metadata": {},
   "source": [
    "##### Low search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a635f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_validation_results = {\n",
    "    example_name: None for example_name in policies_to_validate.keys()\n",
    "}\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    policy_validation_results[example_name], citations[example_name] = (\n",
    "        search_on_perplexity(\n",
    "            policy=example_policy,\n",
    "            model_name=\"sonar-pro\",\n",
    "            search_context_size=\"low\",\n",
    "            system_prompt=system_prompt,\n",
    "            perplexity_api_key=perplexity_api_key,\n",
    "            allowed_sources=allowed_sources,\n",
    "        )\n",
    "    )\n",
    "policy_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fac6d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e12f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(Markdown(policy_validation_results[example_name].reasoning))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a647eb",
   "metadata": {},
   "source": [
    "Wow this already worked very well, even in the cheapest setting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff4fc17",
   "metadata": {},
   "source": [
    "##### Medium search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83036381",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_validation_results = {\n",
    "    example_name: None for example_name in policies_to_validate.keys()\n",
    "}\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    policy_validation_results[example_name], citations[example_name] = (\n",
    "        search_on_perplexity(\n",
    "            policy=example_policy,\n",
    "            model_name=\"sonar-pro\",\n",
    "            search_context_size=\"medium\",\n",
    "            system_prompt=system_prompt,\n",
    "            perplexity_api_key=perplexity_api_key,\n",
    "            allowed_sources=allowed_sources,\n",
    "        )\n",
    "    )\n",
    "policy_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ec1217",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0986c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(Markdown(policy_validation_results[example_name].reasoning))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f67f2ad",
   "metadata": {},
   "source": [
    "##### High search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348c98c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_validation_results = {\n",
    "    example_name: None for example_name in policies_to_validate.keys()\n",
    "}\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    policy_validation_results[example_name], citations[example_name] = (\n",
    "        search_on_perplexity(\n",
    "            policy=example_policy,\n",
    "            model_name=\"sonar-pro\",\n",
    "            search_context_size=\"high\",\n",
    "            system_prompt=system_prompt,\n",
    "            perplexity_api_key=perplexity_api_key,\n",
    "            allowed_sources=allowed_sources,\n",
    "        )\n",
    "    )\n",
    "policy_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec29011",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba9a698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(Markdown(policy_validation_results[example_name].reasoning))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8278dbc",
   "metadata": {},
   "source": [
    "#### Sonar Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faaf971",
   "metadata": {},
   "source": [
    "##### Low search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20f4062",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_validation_results = {\n",
    "    example_name: None for example_name in policies_to_validate.keys()\n",
    "}\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    policy_validation_results[example_name], citations[example_name] = (\n",
    "        search_on_perplexity(\n",
    "            policy=example_policy,\n",
    "            model_name=\"sonar-reasoning\",\n",
    "            search_context_size=\"low\",\n",
    "            system_prompt=system_prompt,\n",
    "            perplexity_api_key=perplexity_api_key,\n",
    "            allowed_sources=allowed_sources,\n",
    "        )\n",
    "    )\n",
    "policy_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e12f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29154bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(Markdown(policy_validation_results[example_name].reasoning))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51e5e80",
   "metadata": {},
   "source": [
    "##### Medium search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cfcb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_validation_results = {\n",
    "    example_name: None for example_name in policies_to_validate.keys()\n",
    "}\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    policy_validation_results[example_name], citations[example_name] = (\n",
    "        search_on_perplexity(\n",
    "            policy=example_policy,\n",
    "            model_name=\"sonar-reasoning\",\n",
    "            search_context_size=\"medium\",\n",
    "            system_prompt=system_prompt,\n",
    "            perplexity_api_key=perplexity_api_key,\n",
    "            allowed_sources=allowed_sources,\n",
    "        )\n",
    "    )\n",
    "policy_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3093487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee35ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(Markdown(policy_validation_results[example_name].reasoning))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1caed7",
   "metadata": {},
   "source": [
    "##### High search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa380d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_validation_results = {\n",
    "    example_name: None for example_name in policies_to_validate.keys()\n",
    "}\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    policy_validation_results[example_name], citations[example_name] = (\n",
    "        search_on_perplexity(\n",
    "            policy=example_policy,\n",
    "            model_name=\"sonar-reasoning\",\n",
    "            search_context_size=\"high\",\n",
    "            system_prompt=system_prompt,\n",
    "            perplexity_api_key=perplexity_api_key,\n",
    "            allowed_sources=allowed_sources,\n",
    "        )\n",
    "    )\n",
    "policy_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0668b68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec9a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(Markdown(policy_validation_results[example_name].reasoning))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa041c7d",
   "metadata": {},
   "source": [
    "#### Sonar Reasoning Pro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666abb97",
   "metadata": {},
   "source": [
    "##### Low search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556c407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_validation_results = {\n",
    "    example_name: None for example_name in policies_to_validate.keys()\n",
    "}\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    policy_validation_results[example_name], citations[example_name] = (\n",
    "        search_on_perplexity(\n",
    "            policy=example_policy,\n",
    "            model_name=\"sonar-reasoning-pro\",\n",
    "            search_context_size=\"low\",\n",
    "            system_prompt=system_prompt,\n",
    "            perplexity_api_key=perplexity_api_key,\n",
    "            allowed_sources=allowed_sources,\n",
    "        )\n",
    "    )\n",
    "policy_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30b0135",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e3e2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(Markdown(policy_validation_results[example_name].reasoning))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763fd457",
   "metadata": {},
   "source": [
    "##### Medium search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69c5535",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_validation_results = {\n",
    "    example_name: None for example_name in policies_to_validate.keys()\n",
    "}\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    policy_validation_results[example_name], citations[example_name] = (\n",
    "        search_on_perplexity(\n",
    "            policy=example_policy,\n",
    "            model_name=\"sonar-reasoning-pro\",\n",
    "            search_context_size=\"medium\",\n",
    "            system_prompt=system_prompt,\n",
    "            perplexity_api_key=perplexity_api_key,\n",
    "            allowed_sources=allowed_sources,\n",
    "        )\n",
    "    )\n",
    "policy_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbce910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2f193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(Markdown(policy_validation_results[example_name].reasoning))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae91c28",
   "metadata": {},
   "source": [
    "##### High search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f614669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_validation_results = {\n",
    "    example_name: None for example_name in policies_to_validate.keys()\n",
    "}\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    policy_validation_results[example_name], citations[example_name] = (\n",
    "        search_on_perplexity(\n",
    "            policy=example_policy,\n",
    "            model_name=\"sonar-reasoning-pro\",\n",
    "            search_context_size=\"high\",\n",
    "            system_prompt=system_prompt,\n",
    "            perplexity_api_key=perplexity_api_key,\n",
    "            allowed_sources=allowed_sources,\n",
    "        )\n",
    "    )\n",
    "policy_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e931f8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0918a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(Markdown(policy_validation_results[example_name].reasoning))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c76112",
   "metadata": {},
   "source": [
    "#### Sonar Deep Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5920b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_validation_results = {\n",
    "    example_name: None for example_name in policies_to_validate.keys()\n",
    "}\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    policy_validation_results[example_name], citations[example_name] = (\n",
    "        search_on_perplexity(\n",
    "            policy=example_policy,\n",
    "            model_name=\"sonar-deep-research\",\n",
    "            system_prompt=system_prompt,\n",
    "            perplexity_api_key=perplexity_api_key,\n",
    "        )\n",
    "    )\n",
    "policy_validation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9209f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab966f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(Markdown(policy_validation_results[example_name].reasoning))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97f5026",
   "metadata": {},
   "source": [
    "### Implemented solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b553e793",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
