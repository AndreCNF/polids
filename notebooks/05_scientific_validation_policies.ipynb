{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a038bd6",
   "metadata": {},
   "source": [
    "# Scientific validation of policy proposals\n",
    "---\n",
    "Experimenting with web search APIs for scientific validation of policy proposals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f82055",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68012ac",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dbf819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, Dict\n",
    "import json\n",
    "import backoff\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "from tqdm.auto import tqdm\n",
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from openai import OpenAI\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dadd7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from polids.config import settings\n",
    "from polids.scientific_validation.perplexity import PerplexityScientificValidator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e316f2c6",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6539d528",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"**Objective:** Analyze the provided policy proposal to determine its scientific validity based on credible, current evidence obtained through web search. Produce a structured JSON output adhering to the `ScientificValidation` schema.\n",
    "\n",
    "**Core Task:** Evaluate the scientific backing for the policy proposal. Your analysis must focus on:\n",
    "\n",
    "1.  **Evidence Assessment:**\n",
    "    - Determine if the **balance of scientific evidence** supports or refutes the policy's likely effectiveness or impact. This directly informs the `is_policy_supported_by_scientific_evidence` field.\n",
    "    - Critically evaluate the **degree of consensus** among reliable scientific sources. Is there broad agreement, significant debate, or insufficient evidence? This directly informs the `is_scientific_consensus_present` field. Remember, `True` requires near-unanimous agreement among sources on the *validation outcome* (supported or not supported).\n",
    "\n",
    "2.  **Source Prioritization:**\n",
    "    - **Highest Priority:** Peer-reviewed scientific studies (especially systematic reviews, meta-analyses, RCTs), reports from established scientific organizations and governmental research bodies.\n",
    "    - **Lower Priority:** Reputable news articles reporting on scientific findings (use mainly for context or pointers to primary sources, verify claims against primary literature).\n",
    "    - **Avoid:** Opinion pieces, anecdotal evidence, single studies contradicted by broader evidence, non-credible sources.\n",
    "\n",
    "3.  **Reasoning Formulation (`validation_reasoning` field):**\n",
    "    - Provide a detailed explanation justifying the boolean field values (`is_policy_supported_by_scientific_evidence`, `is_scientific_consensus_present`).\n",
    "    - Summarize the key findings from the most credible sources.\n",
    "    - Explicitly mention supporting *and* conflicting evidence if found.\n",
    "    - Reference specific evidence or sources briefly (e.g., \"Smith et al., 2021 study showed X,\" \"IPCC report indicates Y\").\n",
    "\n",
    "**Constraints & Guidelines:**\n",
    "- Base your analysis solely on information retrieved from the web search.\n",
    "- Focus on the most current and relevant scientific data.\n",
    "- If evidence is mixed or limited, clearly state this in the reasoning and set boolean flags accordingly (e.g., `is_scientific_consensus_present` would likely be `False`).\n",
    "- Present the final output (the structured JSON) in English, regardless of the policy proposal's original language.\n",
    "- Do not include conversational text before or after the JSON output. Just provide the JSON object matching the `ScientificValidation` schema.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170cbe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_retries = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d2a37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "perplexity_api_key = os.getenv(\"PERPLEXITY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93575d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_sources = [\n",
    "    # --- General Knowledge ---\n",
    "    # Sources offering broad, encyclopedic information.\n",
    "    \"wikipedia.org\",  # Crowd-sourced general knowledge encyclopedia\n",
    "    # --- Data Aggregators ---\n",
    "    # Platforms specializing in collecting, analyzing, and visualizing data on various topics.\n",
    "    \"ourworldindata.org\",  # Accessible global data visualization & analysis\n",
    "    # --- International Organizations ---\n",
    "    # Official websites of major international bodies providing data, reports, and policy guidelines.\n",
    "    \"oecd.org\",  # Organisation for Economic Co-operation and Development data & reports\n",
    "    \"un.org\",  # United Nations reports & policy guidelines (global issues)\n",
    "    \"worldbank.org\",  # World Bank global economic & development data\n",
    "    # --- Research & Policy Analysis Institutes ---\n",
    "    # Organizations focused on specific research areas, often influencing policy.\n",
    "    \"nber.org\",  # National Bureau of Economic Research (influential economics)\n",
    "    # --- Research Aggregators & Databases ---\n",
    "    # Platforms providing access to collections of academic research papers.\n",
    "    \"core.ac.uk\",  # Aggregator for open access research papers (multidisciplinary)\n",
    "    \"ncbi.nlm.nih.gov\",  # National Center for Biotechnology Information (biomedical literature)\n",
    "    \"arxiv.org\",  # Open access preprints (physics, math, CS, quantitative biology, etc.)\n",
    "    \"sci-hub.box\",  # Tool for accessing paywalled scientific papers (legality varies)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380113cc",
   "metadata": {},
   "source": [
    "## Load policies to validate\n",
    "We're going to start from manually defined policies, so as to avoid dependencies on previous steps of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed47a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "policies_to_validate = {\n",
    "    \"carbon_tax\": \"Implementing a carbon tax to reduce greenhouse gas emissions.\",\n",
    "    \"vaccines\": \"Mandatory vaccination for all school-aged children to prevent outbreaks of infectious diseases.\",\n",
    "    \"ubi\": \"Implementing universal basic income to address income inequality and support job displacement due to automation.\",\n",
    "    \"immigration_jobs\": \"Reducing immigration quotas to improve job opportunities for native citizens.\",\n",
    "    \"immigration_crime\": \"Blocking immigration from countries with different cultural backgrounds to reduce crime rates.\",\n",
    "}\n",
    "# Dictionary where to store the results\n",
    "boilerplate_values = {\n",
    "    example_name: None for example_name in policies_to_validate.keys()\n",
    "}\n",
    "policy_validation_results = {\n",
    "    \"gpt4o_mini_low\": boilerplate_values.copy(),\n",
    "    \"gpt4o_mini_medium\": boilerplate_values.copy(),\n",
    "    \"gpt4o_mini_high\": boilerplate_values.copy(),\n",
    "    \"gpt4o_low\": boilerplate_values.copy(),\n",
    "    \"gpt4o_medium\": boilerplate_values.copy(),\n",
    "    \"gpt4o_high\": boilerplate_values.copy(),\n",
    "    \"sonar_low\": boilerplate_values.copy(),\n",
    "    \"sonar_medium\": boilerplate_values.copy(),\n",
    "    \"sonar_high\": boilerplate_values.copy(),\n",
    "    \"sonar_pro_low\": boilerplate_values.copy(),\n",
    "    \"sonar_pro_medium\": boilerplate_values.copy(),\n",
    "    \"sonar_pro_high\": boilerplate_values.copy(),\n",
    "    \"sonar_reasoning_low\": boilerplate_values.copy(),\n",
    "    \"sonar_reasoning_medium\": boilerplate_values.copy(),\n",
    "    \"sonar_reasoning_high\": boilerplate_values.copy(),\n",
    "    \"sonar_reasoning_pro_low\": boilerplate_values.copy(),\n",
    "    \"sonar_reasoning_pro_medium\": boilerplate_values.copy(),\n",
    "    \"sonar_reasoning_pro_high\": boilerplate_values.copy(),\n",
    "    \"sonar_deep_research\": boilerplate_values.copy(),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe970012",
   "metadata": {},
   "source": [
    "## Define the output schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbefc42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScientificValidation(BaseModel):\n",
    "    is_policy_supported_by_scientific_evidence: bool = Field(\n",
    "        description=(\n",
    "            \"Indicates whether the policy proposal is supported by scientific evidence from the searched sources. \"\n",
    "            \"Set to `True` if the majority of reliable sources (e.g., peer-reviewed studies, reports from reputable organizations) \"\n",
    "            \"provide evidence or arguments in favor of the policy's effectiveness or benefits. \"\n",
    "            \"Set to `False` if the majority of sources oppose the policy, find it ineffective, or lack evidence to support it. \"\n",
    "            \"Example: If a policy proposes a carbon tax to reduce emissions, set to `True` if most studies show carbon taxes reduce emissions.\"\n",
    "        )\n",
    "    )\n",
    "    is_scientific_consensus_present: bool = Field(\n",
    "        description=(\n",
    "            \"Indicates whether there is a clear consensus among reliable scientific sources regarding the policy's effectiveness or impact. \"\n",
    "            \"Set to `True` ONLY if nearly all credible sources (e.g., peer-reviewed papers, expert analyses from trusted institutions) \"\n",
    "            \"agree on whether the policy is supported or opposed (i.e., minimal conflicting evidence or opinions). \"\n",
    "            \"Set to `False` if there is significant disagreement, mixed findings, or insufficient data among sources. \"\n",
    "            \"Example: If 9 out of 10 studies agree a policy works, set to `True`. If only 6 out of 10 agree, set to `False`.\"\n",
    "        )\n",
    "    )\n",
    "    validation_reasoning: str = Field(\n",
    "        description=(\n",
    "            \"A detailed explanation of the validation outcome. Include: \"\n",
    "            \"1. A summary of the key evidence or arguments from the sources regarding the policy's effectiveness or impact. \"\n",
    "            \"2. Specific references to the sources (e.g., study titles, authors, faculties, organizations, etc) to support the conclusions. \"\n",
    "            \"3. An explanation of why `is_policy_supported_by_scientific_evidence` and `is_scientific_consensus_present` were set to their respective values, \"\n",
    "            \"including any conflicting evidence if present. \"\n",
    "            \"Example: 'Most studies (e.g., Smith et al., 2020) support the policy due to evidence of reduced emissions by 20%, so `is_policy_supported_by_scientific_evidence` is `True`. \"\n",
    "            \"However, two studies disagree on long-term effects, so `is_scientific_consensus_present` is `False`.'\"\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b51150",
   "metadata": {},
   "source": [
    "## Test different search APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e546428b",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9378de62",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(api_key=settings.openai_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7494e588",
   "metadata": {},
   "source": [
    "#### GPT 4o mini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0006d7e7",
   "metadata": {},
   "source": [
    "##### Low search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b81f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_name = \"gpt4o_mini_low\"\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini-search-preview\",\n",
    "        web_search_options={\n",
    "            \"search_context_size\": \"low\",\n",
    "        },\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": example_policy,\n",
    "            },\n",
    "        ],\n",
    "        response_format=ScientificValidation,  # Specify the schema for the structured output\n",
    "    )\n",
    "    policy_validation_results[mode_name][example_name] = completion.choices[\n",
    "        0\n",
    "    ].message.parsed\n",
    "    assert isinstance(\n",
    "        policy_validation_results[mode_name][example_name], ScientificValidation\n",
    "    ), \"Output does not match the expected schema.\"\n",
    "    citations[example_name] = completion.choices[0].message.annotations\n",
    "policy_validation_results[mode_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4ae459",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef6108f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(\n",
    "    Markdown(policy_validation_results[mode_name][example_name].validation_reasoning)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fbd04a",
   "metadata": {},
   "source": [
    "Seems like GPT 4o mini with low search often doesn't use any search results at all. When it does, I'm only seeing two citations. This is not enough to validate a policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c150b0",
   "metadata": {},
   "source": [
    "##### Medium search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10fe4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_name = \"gpt4o_mini_medium\"\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini-search-preview\",\n",
    "        web_search_options={\n",
    "            \"search_context_size\": \"medium\",\n",
    "        },\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": example_policy,\n",
    "            },\n",
    "        ],\n",
    "        response_format=ScientificValidation,  # Specify the schema for the structured output\n",
    "    )\n",
    "    policy_validation_results[mode_name][example_name] = completion.choices[\n",
    "        0\n",
    "    ].message.parsed\n",
    "    assert isinstance(\n",
    "        policy_validation_results[mode_name][example_name], ScientificValidation\n",
    "    ), \"Output does not match the expected schema.\"\n",
    "    citations[example_name] = completion.choices[0].message.annotations\n",
    "policy_validation_results[mode_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c810fd33",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861a95a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(\n",
    "    Markdown(policy_validation_results[mode_name][example_name].validation_reasoning)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d1d01b",
   "metadata": {},
   "source": [
    "Medium gets more sources for some of the samples, but still has some of them without any citations. This is not enough to validate a policy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d7f3b8",
   "metadata": {},
   "source": [
    "##### High search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf02692",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_name = \"gpt4o_mini_high\"\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini-search-preview\",\n",
    "        web_search_options={\n",
    "            \"search_context_size\": \"high\",\n",
    "        },\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": example_policy,\n",
    "            },\n",
    "        ],\n",
    "        response_format=ScientificValidation,  # Specify the schema for the structured output\n",
    "    )\n",
    "    policy_validation_results[mode_name][example_name] = completion.choices[\n",
    "        0\n",
    "    ].message.parsed\n",
    "    assert isinstance(\n",
    "        policy_validation_results[mode_name][example_name], ScientificValidation\n",
    "    ), \"Output does not match the expected schema.\"\n",
    "    citations[example_name] = completion.choices[0].message.annotations\n",
    "policy_validation_results[mode_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b895b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3a4006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(\n",
    "    Markdown(policy_validation_results[mode_name][example_name].validation_reasoning)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8afb355",
   "metadata": {},
   "source": [
    "Still some samples without citations, even on high search context 👎🏻"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45265eb4",
   "metadata": {},
   "source": [
    "#### GPT 4o (larger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0d5f76",
   "metadata": {},
   "source": [
    "##### Low search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f7dce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_name = \"gpt4o_low\"\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-search-preview\",\n",
    "        web_search_options={\n",
    "            \"search_context_size\": \"low\",\n",
    "        },\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": example_policy,\n",
    "            },\n",
    "        ],\n",
    "        response_format=ScientificValidation,  # Specify the schema for the structured output\n",
    "    )\n",
    "    policy_validation_results[mode_name][example_name] = completion.choices[\n",
    "        0\n",
    "    ].message.parsed\n",
    "    assert isinstance(\n",
    "        policy_validation_results[mode_name][example_name], ScientificValidation\n",
    "    ), \"Output does not match the expected schema.\"\n",
    "    citations[example_name] = completion.choices[0].message.annotations\n",
    "policy_validation_results[mode_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d72ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e01d01e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(\n",
    "    Markdown(policy_validation_results[mode_name][example_name].validation_reasoning)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820dd8fa",
   "metadata": {},
   "source": [
    "##### Medium search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3093c581",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_name = \"gpt4o_medium\"\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-search-preview\",\n",
    "        web_search_options={\n",
    "            \"search_context_size\": \"medium\",\n",
    "        },\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": example_policy,\n",
    "            },\n",
    "        ],\n",
    "        response_format=ScientificValidation,  # Specify the schema for the structured output\n",
    "    )\n",
    "    policy_validation_results[mode_name][example_name] = completion.choices[\n",
    "        0\n",
    "    ].message.parsed\n",
    "    assert isinstance(\n",
    "        policy_validation_results[mode_name][example_name], ScientificValidation\n",
    "    ), \"Output does not match the expected schema.\"\n",
    "    citations[example_name] = completion.choices[0].message.annotations\n",
    "policy_validation_results[mode_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90368961",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554c3cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(\n",
    "    Markdown(policy_validation_results[mode_name][example_name].validation_reasoning)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c86123",
   "metadata": {},
   "source": [
    "##### High search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab8982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_name = \"gpt4o_high\"\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-search-preview\",\n",
    "        web_search_options={\n",
    "            \"search_context_size\": \"high\",\n",
    "        },\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_prompt,\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": example_policy,\n",
    "            },\n",
    "        ],\n",
    "        response_format=ScientificValidation,  # Specify the schema for the structured output\n",
    "    )\n",
    "    policy_validation_results[mode_name][example_name] = completion.choices[\n",
    "        0\n",
    "    ].message.parsed\n",
    "    assert isinstance(\n",
    "        policy_validation_results[mode_name][example_name], ScientificValidation\n",
    "    ), \"Output does not match the expected schema.\"\n",
    "    citations[example_name] = completion.choices[0].message.annotations\n",
    "policy_validation_results[mode_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df6c492",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9785b324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(\n",
    "    Markdown(policy_validation_results[mode_name][example_name].validation_reasoning)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84dab79",
   "metadata": {},
   "source": [
    "Citations often rely on news articles and Wikipedia, not actual research."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a027885c",
   "metadata": {},
   "source": [
    "### Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77934787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_valid_json(response: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extracts and returns only the valid JSON part from a response object.\n",
    "\n",
    "    This function assumes that the response has a structure where the valid JSON\n",
    "    is included in the 'content' field of the first choice's message, after the\n",
    "    closing \"</think>\" marker. Any markdown code fences (e.g. ```json) are stripped.\n",
    "\n",
    "    Parameters:\n",
    "        response (dict): The full API response object.\n",
    "\n",
    "    Returns:\n",
    "        dict: The parsed JSON object extracted from the content.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If no valid JSON can be parsed from the content.\n",
    "    \"\"\"\n",
    "    # Navigate to the 'content' field; adjust if your structure differs.\n",
    "    content = response.get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\")\n",
    "\n",
    "    # Find the index of the closing </think> tag.\n",
    "    marker = \"</think>\"\n",
    "    idx = content.rfind(marker)\n",
    "\n",
    "    if idx == -1:\n",
    "        # If marker not found, try parsing the entire content.\n",
    "        try:\n",
    "            return json.loads(content)\n",
    "        except json.JSONDecodeError as e:\n",
    "            raise ValueError(\n",
    "                \"No </think> marker found and content is not valid JSON\"\n",
    "            ) from e\n",
    "\n",
    "    # Extract the substring after the marker.\n",
    "    json_str = content[idx + len(marker) :].strip()\n",
    "\n",
    "    # Remove markdown code fence markers if present.\n",
    "    if json_str.startswith(\"```json\"):\n",
    "        json_str = json_str[len(\"```json\") :].strip()\n",
    "    if json_str.startswith(\"```\"):\n",
    "        json_str = json_str[3:].strip()\n",
    "    if json_str.endswith(\"```\"):\n",
    "        json_str = json_str[:-3].strip()\n",
    "\n",
    "    try:\n",
    "        parsed_json = json.loads(json_str)\n",
    "        return parsed_json\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(\"Failed to parse valid JSON from response content\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188bce15",
   "metadata": {},
   "outputs": [],
   "source": [
    "@backoff.on_exception(\n",
    "    backoff.expo,\n",
    "    ValidationError,\n",
    "    max_tries=max_retries,\n",
    "    max_time=60,\n",
    ")\n",
    "def search_on_perplexity(\n",
    "    policy: str,\n",
    "    model_name: str,\n",
    "    search_context_size: str = None,\n",
    "    system_prompt: str = system_prompt,\n",
    "    perplexity_api_key: str = perplexity_api_key,\n",
    "    allowed_sources: list = None,\n",
    ") -> tuple[ScientificValidation, list]:\n",
    "    \"\"\"\n",
    "    Search for scientific validation of a policy proposal using Perplexity AI.\n",
    "\n",
    "    Args:\n",
    "        policy (str): The policy proposal to validate.\n",
    "        model_name (str): The model name to use for the search.\n",
    "        search_context_size (str, optional): The size of the search context (low, medium, high).\n",
    "        system_prompt (str): The system prompt for the model.\n",
    "        perplexity_api_key (str): The API key for Perplexity AI.\n",
    "        allowed_sources (list, optional): A list of allowed sources for the search.\n",
    "\n",
    "    Returns:\n",
    "        tuple[ScientificValidation, list]: A tuple containing the validation result and citations.\n",
    "    \"\"\"\n",
    "    request_payload = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": policy},\n",
    "        ],\n",
    "        \"response_format\": {\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\"schema\": ScientificValidation.model_json_schema()},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    if allowed_sources:\n",
    "        # Only search on the allowed sources\n",
    "        request_payload[\"search_domain_filter\"] = allowed_sources\n",
    "\n",
    "    if search_context_size:\n",
    "        # Define how many sources to use for the search\n",
    "        assert search_context_size in [\"low\", \"medium\", \"high\"], (\n",
    "            f\"Invalid search context size: {search_context_size}. \"\n",
    "            \"Must be one of: low, medium, high.\"\n",
    "        )\n",
    "        request_payload[\"web_search_options\"] = {\n",
    "            \"search_context_size\": search_context_size\n",
    "        }\n",
    "\n",
    "    response = requests.post(\n",
    "        \"https://api.perplexity.ai/chat/completions\",\n",
    "        headers={\"Authorization\": f\"Bearer {perplexity_api_key}\"},\n",
    "        json=request_payload,\n",
    "    ).json()\n",
    "\n",
    "    citations = response.get(\"citations\", [])\n",
    "    response_content = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    if (\"reasoning\" in model_name) or (\"<think>\" in response_content):\n",
    "        # Extract the valid JSON part from the response content\n",
    "        json_content = extract_valid_json(response)\n",
    "        # Parse the JSON content into the ScientificValidation model\n",
    "        parsed_content = ScientificValidation.model_validate(json_content)\n",
    "    else:\n",
    "        # Parse the string content into the ScientificValidation model\n",
    "        parsed_content = ScientificValidation.model_validate_json(response_content)\n",
    "\n",
    "    return parsed_content, citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e614c0ad",
   "metadata": {},
   "source": [
    "#### Sonar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7328b30d",
   "metadata": {},
   "source": [
    "##### Low search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f81589",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_name = \"sonar_low\"\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    policy_validation_results[mode_name][example_name], citations[example_name] = (\n",
    "        search_on_perplexity(\n",
    "            policy=example_policy,\n",
    "            model_name=\"sonar\",\n",
    "            search_context_size=\"low\",\n",
    "            system_prompt=system_prompt,\n",
    "            perplexity_api_key=perplexity_api_key,\n",
    "            allowed_sources=allowed_sources,\n",
    "        )\n",
    "    )\n",
    "policy_validation_results[mode_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78cc2184",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57088c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(\n",
    "    Markdown(policy_validation_results[mode_name][example_name].validation_reasoning)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dc05a3",
   "metadata": {},
   "source": [
    "Wow this already worked very well, even in the cheapest setting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6046c589",
   "metadata": {},
   "source": [
    "##### Medium search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189feb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_name = \"sonar_medium\"\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    policy_validation_results[mode_name][example_name], citations[example_name] = (\n",
    "        search_on_perplexity(\n",
    "            policy=example_policy,\n",
    "            model_name=\"sonar\",\n",
    "            search_context_size=\"medium\",\n",
    "            system_prompt=system_prompt,\n",
    "            perplexity_api_key=perplexity_api_key,\n",
    "            allowed_sources=allowed_sources,\n",
    "        )\n",
    "    )\n",
    "policy_validation_results[mode_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c274309f",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd3ef70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(\n",
    "    Markdown(policy_validation_results[mode_name][example_name].validation_reasoning)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dde98af",
   "metadata": {},
   "source": [
    "##### High search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb287b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_name = \"sonar_high\"\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    policy_validation_results[mode_name][example_name], citations[example_name] = (\n",
    "        search_on_perplexity(\n",
    "            policy=example_policy,\n",
    "            model_name=\"sonar\",\n",
    "            search_context_size=\"high\",\n",
    "            system_prompt=system_prompt,\n",
    "            perplexity_api_key=perplexity_api_key,\n",
    "            allowed_sources=allowed_sources,\n",
    "        )\n",
    "    )\n",
    "policy_validation_results[mode_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ff672e",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f08d4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(\n",
    "    Markdown(policy_validation_results[mode_name][example_name].validation_reasoning)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6724095e",
   "metadata": {},
   "source": [
    "Using higher search context seems very useful to provide a broader view to the LLM and make it understand better if a policy proposal is consensual or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c298c4",
   "metadata": {},
   "source": [
    "#### Sonar Pro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3526d97b",
   "metadata": {},
   "source": [
    "##### Low search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a635f0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_name = \"sonar_pro_low\"\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    policy_validation_results[mode_name][example_name], citations[example_name] = (\n",
    "        search_on_perplexity(\n",
    "            policy=example_policy,\n",
    "            model_name=\"sonar-pro\",\n",
    "            search_context_size=\"low\",\n",
    "            system_prompt=system_prompt,\n",
    "            perplexity_api_key=perplexity_api_key,\n",
    "            allowed_sources=allowed_sources,\n",
    "        )\n",
    "    )\n",
    "policy_validation_results[mode_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fac6d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e12f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(\n",
    "    Markdown(policy_validation_results[mode_name][example_name].validation_reasoning)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a647eb",
   "metadata": {},
   "source": [
    "Wow this already worked very well, even in the cheapest setting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff4fc17",
   "metadata": {},
   "source": [
    "##### Medium search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83036381",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_name = \"sonar_pro_medium\"\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    policy_validation_results[mode_name][example_name], citations[example_name] = (\n",
    "        search_on_perplexity(\n",
    "            policy=example_policy,\n",
    "            model_name=\"sonar-pro\",\n",
    "            search_context_size=\"medium\",\n",
    "            system_prompt=system_prompt,\n",
    "            perplexity_api_key=perplexity_api_key,\n",
    "            allowed_sources=allowed_sources,\n",
    "        )\n",
    "    )\n",
    "policy_validation_results[mode_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ec1217",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0986c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(\n",
    "    Markdown(policy_validation_results[mode_name][example_name].validation_reasoning)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f67f2ad",
   "metadata": {},
   "source": [
    "##### High search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348c98c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_name = \"sonar_pro_high\"\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    policy_validation_results[mode_name][example_name], citations[example_name] = (\n",
    "        search_on_perplexity(\n",
    "            policy=example_policy,\n",
    "            model_name=\"sonar-pro\",\n",
    "            search_context_size=\"high\",\n",
    "            system_prompt=system_prompt,\n",
    "            perplexity_api_key=perplexity_api_key,\n",
    "            allowed_sources=allowed_sources,\n",
    "        )\n",
    "    )\n",
    "policy_validation_results[mode_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec29011",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba9a698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(\n",
    "    Markdown(policy_validation_results[mode_name][example_name].validation_reasoning)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97e08f4",
   "metadata": {},
   "source": [
    "The Pro version of Sonar has more detailed and useful explanations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8278dbc",
   "metadata": {},
   "source": [
    "#### Sonar Reasoning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0faaf971",
   "metadata": {},
   "source": [
    "##### Low search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20f4062",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_name = \"sonar_reasoning_low\"\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    policy_validation_results[mode_name][example_name], citations[example_name] = (\n",
    "        search_on_perplexity(\n",
    "            policy=example_policy,\n",
    "            model_name=\"sonar-reasoning\",\n",
    "            search_context_size=\"low\",\n",
    "            system_prompt=system_prompt,\n",
    "            perplexity_api_key=perplexity_api_key,\n",
    "            allowed_sources=allowed_sources,\n",
    "        )\n",
    "    )\n",
    "policy_validation_results[mode_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e12f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29154bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(\n",
    "    Markdown(policy_validation_results[mode_name][example_name].validation_reasoning)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51e5e80",
   "metadata": {},
   "source": [
    "##### Medium search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cfcb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_name = \"sonar_reasoning_medium\"\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    policy_validation_results[mode_name][example_name], citations[example_name] = (\n",
    "        search_on_perplexity(\n",
    "            policy=example_policy,\n",
    "            model_name=\"sonar-reasoning\",\n",
    "            search_context_size=\"medium\",\n",
    "            system_prompt=system_prompt,\n",
    "            perplexity_api_key=perplexity_api_key,\n",
    "            allowed_sources=allowed_sources,\n",
    "        )\n",
    "    )\n",
    "policy_validation_results[mode_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3093487d",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee35ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(\n",
    "    Markdown(policy_validation_results[mode_name][example_name].validation_reasoning)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1caed7",
   "metadata": {},
   "source": [
    "##### High search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa380d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_name = \"sonar_reasoning_medium\"\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    policy_validation_results[mode_name][example_name], citations[example_name] = (\n",
    "        search_on_perplexity(\n",
    "            policy=example_policy,\n",
    "            model_name=\"sonar-reasoning\",\n",
    "            search_context_size=\"high\",\n",
    "            system_prompt=system_prompt,\n",
    "            perplexity_api_key=perplexity_api_key,\n",
    "            allowed_sources=allowed_sources,\n",
    "        )\n",
    "    )\n",
    "policy_validation_results[mode_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0668b68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec9a03e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(\n",
    "    Markdown(policy_validation_results[mode_name][example_name].validation_reasoning)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa041c7d",
   "metadata": {},
   "source": [
    "#### Sonar Reasoning Pro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666abb97",
   "metadata": {},
   "source": [
    "##### Low search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556c407a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_name = \"sonar_reasoning_pro_low\"\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    policy_validation_results[mode_name][example_name], citations[example_name] = (\n",
    "        search_on_perplexity(\n",
    "            policy=example_policy,\n",
    "            model_name=\"sonar-reasoning-pro\",\n",
    "            search_context_size=\"low\",\n",
    "            system_prompt=system_prompt,\n",
    "            perplexity_api_key=perplexity_api_key,\n",
    "            allowed_sources=allowed_sources,\n",
    "        )\n",
    "    )\n",
    "policy_validation_results[mode_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30b0135",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e3e2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(\n",
    "    Markdown(policy_validation_results[mode_name][example_name].validation_reasoning)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763fd457",
   "metadata": {},
   "source": [
    "##### Medium search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69c5535",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_name = \"sonar_reasoning_pro_medium\"\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    policy_validation_results[mode_name][example_name], citations[example_name] = (\n",
    "        search_on_perplexity(\n",
    "            policy=example_policy,\n",
    "            model_name=\"sonar-reasoning-pro\",\n",
    "            search_context_size=\"medium\",\n",
    "            system_prompt=system_prompt,\n",
    "            perplexity_api_key=perplexity_api_key,\n",
    "            allowed_sources=allowed_sources,\n",
    "        )\n",
    "    )\n",
    "policy_validation_results[mode_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbce910d",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2f193c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(\n",
    "    Markdown(policy_validation_results[mode_name][example_name].validation_reasoning)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae91c28",
   "metadata": {},
   "source": [
    "##### High search context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f614669c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_name = \"sonar_reasoning_pro_high\"\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    policy_validation_results[mode_name][example_name], citations[example_name] = (\n",
    "        search_on_perplexity(\n",
    "            policy=example_policy,\n",
    "            model_name=\"sonar-reasoning-pro\",\n",
    "            search_context_size=\"high\",\n",
    "            system_prompt=system_prompt,\n",
    "            perplexity_api_key=perplexity_api_key,\n",
    "            allowed_sources=allowed_sources,\n",
    "        )\n",
    "    )\n",
    "policy_validation_results[mode_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e931f8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0918a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(\n",
    "    Markdown(policy_validation_results[mode_name][example_name].validation_reasoning)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c76112",
   "metadata": {},
   "source": [
    "#### Sonar Deep Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5920b121",
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_name = \"sonar_deep_research\"\n",
    "citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    policy_validation_results[mode_name][example_name], citations[example_name] = (\n",
    "        search_on_perplexity(\n",
    "            policy=example_policy,\n",
    "            model_name=\"sonar-deep-research\",\n",
    "            system_prompt=system_prompt,\n",
    "            perplexity_api_key=perplexity_api_key,\n",
    "        )\n",
    "    )\n",
    "policy_validation_results[mode_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9209f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab966f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of the reasoning, in an easier to read format\n",
    "display(\n",
    "    Markdown(policy_validation_results[mode_name][example_name].validation_reasoning)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2d7b50",
   "metadata": {},
   "source": [
    "Deep research is an interesting tool but, being at least 4x slower than the second slowest option here, without a significant increase in output quality, it seems like an overkill."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53d8a14",
   "metadata": {},
   "source": [
    "### Compare all options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b553e793",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_to_compare = \"immigration_crime\"\n",
    "for mode_name, validation_per_policy in policy_validation_results.items():\n",
    "    if not validation_per_policy[example_to_compare]:\n",
    "        continue\n",
    "    display(\n",
    "        Markdown(f\"\"\"# {example_to_compare} validation for {mode_name}\n",
    "## Is policy supported by sources?\n",
    "{validation_per_policy[example_to_compare].is_policy_supported_by_scientific_evidence}\n",
    "## Is validation consensual and reliable?\n",
    "{validation_per_policy[example_to_compare].is_scientific_consensus_present}\n",
    "## Reasoning\n",
    "{validation_per_policy[example_to_compare].validation_reasoning}\"\"\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cf7871",
   "metadata": {},
   "source": [
    "My personal favourite option is Sonar Pro, whose validation outputs align with what I see in the search results and provides a nicely detailed, balanced reasoning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97f5026",
   "metadata": {},
   "source": [
    "### Implemented solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4bb484",
   "metadata": {},
   "outputs": [],
   "source": [
    "scientific_validator = PerplexityScientificValidator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099b3f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "polids_policy_validation = {\n",
    "    example_name: None for example_name in policies_to_validate.keys()\n",
    "}\n",
    "polids_citations = {example_name: None for example_name in policies_to_validate.keys()}\n",
    "for example_name, example_policy in tqdm(policies_to_validate.items()):\n",
    "    polids_policy_validation[example_name], polids_citations[example_name] = (\n",
    "        scientific_validator.process(policy_proposal=example_policy)\n",
    "    )\n",
    "polids_policy_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b0a41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "polids_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5ef9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
