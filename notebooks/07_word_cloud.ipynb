{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word cloud\n",
    "---\n",
    "\n",
    "Experiment with generating a word cloud from text. It should include a cleanup of the text, excluding stop words and punctuation, as well as setting everything in lowercase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from IPython.display import Markdown, display\n",
    "from tqdm.auto import tqdm\n",
    "from lingua import LanguageDetectorBuilder\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from pydantic import BaseModel, Field\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from polids.config import settings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_annotated_data_path = Path(\"data/portugal_2022/programs/\")\n",
    "# Using this PDF which had issues with stopwords before (e.g. a lot of \"se\")\n",
    "human_annotated_md = human_annotated_data_path / \"be.md\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_content = human_annotated_md.read_text()\n",
    "display(Markdown(markdown_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define lemmatization step\n",
    "\n",
    "Lemmatization is the process of reducing a word to its base or root form. For example, \"running\" becomes \"run\", and \"better\" becomes \"good\". This is useful for word cloud generation as it can group together similar words and reduce the overall number of unique words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI(api_key=settings.openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmatizedWord(BaseModel):\n",
    "    \"\"\"\n",
    "    Represents the lemmatized form of a word, derived from its context and language.\n",
    "    Designed for structured output from an LLM.\n",
    "    \"\"\"\n",
    "\n",
    "    lemma: str = Field(\n",
    "        description=\"The lemmatized (base/dictionary) form of the input word, determined by its context and language rules. If the word is unknown, a proper noun without a standard lemma, or already in its base form, return the original word.\"\n",
    "    )\n",
    "    part_of_speech: str = Field(\n",
    "        description=\"The inferred part of speech (e.g., NOUN, VERB, ADJ, PROPN, UNKNOWN) based on the context that justifies the lemma. Use UNKNOWN if the POS cannot be determined reliably.\",\n",
    "    )\n",
    "\n",
    "\n",
    "def lemmatize_word(\n",
    "    word_to_lemmatize: str,\n",
    "    text_chunk: str,\n",
    "    language: str,\n",
    "    client: OpenAI,\n",
    "    llm_name: str = \"gpt-4.1-nano-2025-04-14\",\n",
    "    temperature: float = 0.0,\n",
    "    seed: int = 42,\n",
    ") -> LemmatizedWord:\n",
    "    \"\"\"\n",
    "    Finds the lemma of a word given its context and language.\n",
    "\n",
    "    Args:\n",
    "        word_to_lemmatize (str): The specific word to lemmatize.\n",
    "        text_chunk (str): The surrounding text context for the word.\n",
    "        language (str): The language of the word and text chunk (e.g., 'English', 'Spanish').\n",
    "        client (OpenAI): An initialized OpenAI client instance.\n",
    "        llm_name (str, optional): The OpenAI GPT-4.1 model to use. Defaults to \"gpt-4.1-nano-2025-04-14\".\n",
    "        temperature (float, optional): Sampling temperature for the model. Defaults to 0.0.\n",
    "        seed (int, optional): Seed for reproducibility. Defaults to 42.\n",
    "\n",
    "    Returns:\n",
    "        LemmatizedWord: A LemmatizedWord object containing the lemma and part of speech.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If an error occurs during LLM processing.\n",
    "        AssertionError: If the output does not match the LemmatizedWord schema.\n",
    "    \"\"\"\n",
    "\n",
    "    # System prompt structured according to GPT-4.1 guide recommendations\n",
    "    system_prompt = \"\"\"\n",
    "# Role and Objective\n",
    "You are an expert multilingual computational linguist. Your objective is to accurately lemmatize a given <word> based on its <context> and <language>, strictly adhering to the provided instructions and output format.\n",
    "\n",
    "# Instructions\n",
    "- Analyze the provided <word> within its surrounding <context>.\n",
    "- Determine the word's intended meaning and part of speech (POS) based *only* on the provided <context> and the rules of the specified <language>.\n",
    "- Output the correct lemma (dictionary/base form) for the <word>.\n",
    "- If the <word> is already in its lemma form (e.g., \"meeting\" used as a noun), return the word itself as the lemma.\n",
    "- If the <word> is a proper noun (e.g., \"London\"), return the word itself as the lemma and identify the POS as 'PROPN'.\n",
    "- If the <word> appears to be an unknown word, typo, or cannot be reliably lemmatized using the context, return the original <word> as the lemma and set POS to 'UNKNOWN'.\n",
    "- The lemmatization must be specific to the given <language>. For example, \"better\" (English) -> \"good\"; \"hablaba\" (Spanish) -> \"hablar\".\n",
    "- Follow the reasoning steps outlined below.\n",
    "- Generate output *only* in the specified structured format. Do not add any extra explanations or conversational text.\n",
    "\n",
    "# Reasoning Steps\n",
    "1.  **Identify Inputs:** Note the specific <word>, <context>, and <language>.\n",
    "2.  **Contextual Analysis:** Read the <context> carefully to understand how the <word> is used.\n",
    "3.  **POS Tagging:** Determine the most likely part of speech (POS) of the <word> in this specific context (e.g., VERB, NOUN, ADJ, ADV, PROPN).\n",
    "4.  **Lemmatization Rule Application:** Apply the lemmatization rules for the identified POS in the specified <language> to find the base/dictionary form (lemma).\n",
    "5.  **Handle Edge Cases:** Check if the word is a proper noun, already a lemma, or unknown/unclear. Adjust lemma and POS accordingly (using 'PROPN' or 'UNKNOWN' for POS if applicable, and returning the original word as lemma in these cases).\n",
    "6.  **Format Output:** Construct the final output strictly according to the `LemmatizedWord` schema.\n",
    "\n",
    "# Output Format\n",
    "Provide the result as a JSON object conforming to the `LemmatizedWord` schema. Ensure the `original_word` field exactly matches the input <word>. The required fields are:\n",
    "- `lemma`: string (The determined lemma)\n",
    "- `part_of_speech`: string or null (e.g., \"NOUN\", \"VERB\", \"ADJ\", \"PROPN\", \"UNKNOWN\")\n",
    "- `original_word`: string (The exact input word)\n",
    "\n",
    "# Examples\n",
    "<example>\n",
    "<input>\n",
    "  <word>running</word>\n",
    "  <context>He is running quickly.</context>\n",
    "  <language>English</language>\n",
    "</input>\n",
    "<output>\n",
    "  {{\n",
    "    \"lemma\": \"run\",\n",
    "    \"part_of_speech\": \"VERB\",\n",
    "    \"original_word\": \"running\"\n",
    "  }}\n",
    "</output>\n",
    "</example>\n",
    "\n",
    "<example>\n",
    "<input>\n",
    "  <word>meetings</word>\n",
    "  <context>We hold weekly meetings.</context>\n",
    "  <language>English</language>\n",
    "</input>\n",
    "<output>\n",
    "  {{\n",
    "    \"lemma\": \"meeting\",\n",
    "    \"part_of_speech\": \"NOUN\",\n",
    "    \"original_word\": \"meetings\"\n",
    "  }}\n",
    "</output>\n",
    "</example>\n",
    "\n",
    "<example>\n",
    "<input>\n",
    "  <word>Paris</word>\n",
    "  <context>She traveled to Paris.</context>\n",
    "  <language>English</language>\n",
    "</input>\n",
    "<output>\n",
    "  {{\n",
    "    \"lemma\": \"Paris\",\n",
    "    \"part_of_speech\": \"PROPN\",\n",
    "    \"original_word\": \"Paris\"\n",
    "  }}\n",
    "</output>\n",
    "</example>\n",
    "\n",
    "<example>\n",
    "<input>\n",
    "  <word>corpora</word>\n",
    "  <context>Linguistic analysis often involves large text corpora.</context>\n",
    "  <language>English</language>\n",
    "</input>\n",
    "<output>\n",
    "  {{\n",
    "    \"lemma\": \"corpus\",\n",
    "    \"part_of_speech\": \"NOUN\",\n",
    "    \"original_word\": \"corpora\"\n",
    "  }}\n",
    "</output>\n",
    "</example>\n",
    "\n",
    "<example>\n",
    "<input>\n",
    "  <word>chevaux</word>\n",
    "  <context>Les chevaux sauvages galopaient.</context>\n",
    "  <language>French</language>\n",
    "</input>\n",
    "<output>\n",
    "  {{\n",
    "    \"lemma\": \"cheval\",\n",
    "    \"part_of_speech\": \"NOUN\",\n",
    "    \"original_word\": \"chevaux\"\n",
    "  }}\n",
    "</output>\n",
    "</example>\n",
    "\n",
    "Now, apply these instructions and reasoning steps meticulously to the user's input.\n",
    "\"\"\"\n",
    "\n",
    "    # User prompt using clear delimiters\n",
    "    user_prompt = f\"\"\"Please perform lemmatization according to the system instructions based on the following details:\n",
    "\n",
    "<input>\n",
    "  <language>{language}</language>\n",
    "  <word>{word_to_lemmatize}</word>\n",
    "  <context>{text_chunk}</context>\n",
    "</input>\n",
    "\n",
    "Provide the output in the specified structured JSON format.\n",
    "\"\"\"\n",
    "\n",
    "    try:\n",
    "        completion = client.beta.chat.completions.parse(\n",
    "            model=llm_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            response_format=LemmatizedWord,  # Enforce the schema\n",
    "            temperature=temperature,\n",
    "            seed=seed,\n",
    "        )\n",
    "\n",
    "        lemmatized_result = completion.choices[0].message.parsed\n",
    "\n",
    "        # Validate the output type\n",
    "        assert isinstance(lemmatized_result, LemmatizedWord), (\n",
    "            f\"Output failed Pydantic validation. Expected LemmatizedWord, \"\n",
    "            f\"got {type(lemmatized_result)}\"\n",
    "        )\n",
    "\n",
    "        return lemmatized_result\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during LLM processing: {e}\")\n",
    "        # Fallback strategy: Return the original word as lemma with UNKNOWN POS\n",
    "        return LemmatizedWord(\n",
    "            lemma=word_to_lemmatize,\n",
    "            part_of_speech=\"UNKNOWN\",  # Indicate failure/uncertainty via POS\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up the text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect the language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include only languages that are not yet extinct (= currently excludes Latin)\n",
    "language_detector = LanguageDetectorBuilder.from_all_spoken_languages().build()\n",
    "language_detection_result = language_detector.detect_language_of(markdown_content)\n",
    "detected_language = language_detection_result.name.lower()\n",
    "print(f\"Detected language: {detected_language}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words and lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(detected_language))\n",
    "print(f\"Stop words in {detected_language}:\\n{', '.join(stop_words)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = word_tokenize(markdown_content)\n",
    "# Lowercase and trim words\n",
    "word_tokens_clean = [w.lower().strip() for w in word_tokens]\n",
    "filtered_words = []\n",
    "for idx, word in tqdm(\n",
    "    enumerate(word_tokens_clean), total=len(word_tokens_clean), desc=\"Processing words\"\n",
    "):\n",
    "    word = word.lower().strip()\n",
    "    # Remove markdown symbols\n",
    "    if word.startswith(\"#\"):\n",
    "        continue\n",
    "    # Remove punctuation\n",
    "    if word in string.punctuation:\n",
    "        continue\n",
    "    # Remove stopwords\n",
    "    if word in stop_words:\n",
    "        continue\n",
    "    # Remove empty strings\n",
    "    if word == \"\":\n",
    "        continue\n",
    "    # Remove words with numbers\n",
    "    if any(char.isdigit() for char in word):\n",
    "        continue\n",
    "    # Get the neighboring words, 10 in total\n",
    "    context_size_in_words = 10\n",
    "    start_idx = max(0, idx - context_size_in_words // 2)\n",
    "    end_idx = min(len(word_tokens_clean), idx + context_size_in_words // 2)\n",
    "    context = \" \".join(word_tokens_clean[start_idx:end_idx])\n",
    "    # Lemmatize the word\n",
    "    lemmatized_word = lemmatize_word(\n",
    "        word_to_lemmatize=word,\n",
    "        text_chunk=context,\n",
    "        language=detected_language,\n",
    "        client=openai_client,\n",
    "    )\n",
    "    # Add the lemmatized word to the list\n",
    "    filtered_words.append(lemmatized_word.lemma)\n",
    "print(f\"Filtered words:\\n{', '.join(filtered_words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_cloud(\n",
    "    words: list[str],\n",
    "    max_words: int = 500,\n",
    "    image_path: str | None = None,\n",
    "    image_name: str | None = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a word cloud based on a set of words.\n",
    "\n",
    "    Args:\n",
    "        words (list[str]):\n",
    "            List of words to be included in the word cloud.\n",
    "        max_words (int):\n",
    "            Maximum number of words to be included in the word cloud.\n",
    "        image_path (str):\n",
    "            Path to the image file where to save the word cloud.\n",
    "        image_name (str):\n",
    "            Name of the image where to save the word cloud.\n",
    "    \"\"\"\n",
    "\n",
    "    # change the value to black\n",
    "    def black_color_func(\n",
    "        word, font_size, position, orientation, random_state=None, **kwargs\n",
    "    ):\n",
    "        return \"hsl(0,100%, 1%)\"\n",
    "\n",
    "    # set the wordcloud background color to white\n",
    "    # set width and height to higher quality, 3000 x 2000\n",
    "    wordcloud = WordCloud(\n",
    "        background_color=\"white\",\n",
    "        width=3000,\n",
    "        height=2000,\n",
    "        max_words=max_words,\n",
    "    ).generate(\" \".join(words))\n",
    "    # set the word color to black\n",
    "    wordcloud.recolor(color_func=black_color_func)\n",
    "    # set the figsize\n",
    "    plt.figure(figsize=[15, 10])\n",
    "    # plot the wordcloud\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    # remove plot axes\n",
    "    plt.axis(\"off\")\n",
    "    if image_path is not None and image_name is not None:\n",
    "        # save the image\n",
    "        plt.savefig(os.path.join(image_path, image_name), bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_word_cloud(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
